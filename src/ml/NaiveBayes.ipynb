{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj49mSrsDUzM",
        "outputId": "c24bfe6a-a0ba-44f1-f872-5e6244905444",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load SST-2 dataset\n",
        "dataset = load_dataset(\"aayya/sst2-augmented\")\n"
      ],
      "metadata": {
        "id": "8wlNQafEDYZC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c01c5afe-3a2d-4e07-fa68-54d56060984a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5PVksH280F33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# View a sample\n",
        "for _, example in zip(range(5), dataset[\"train_orig\"]):\n",
        "  print(example[\"sentence\"], example[\"label\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2p7sRlpEmNa",
        "outputId": "546a8a38-8952-42ed-e294-38d95adc57fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hide new secretions from the parental units  0\n",
            "contains no wit , only labored gags  0\n",
            "that loves its characters and communicates something rather beautiful about human nature  1\n",
            "remains utterly satisfied to remain the same throughout  0\n",
            "on the worst revenge-of-the-nerds clichés the filmmakers could dredge up  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "vzlRzgNH56Lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import string\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def preproc(text: str) -> str:\n",
        "  text = str(text).lower()                            # Lowercase words\n",
        "  text = re.sub(r\"\\[(.*?)\\]\", \"\", text)               # Remove [+XYZ chars] in content\n",
        "  text = re.sub(r\"\\s+\", \" \", text)                    # Remove multiple spaces in content\n",
        "  text = re.sub(r\"\\w+…|…\", \"\", text)                  # Remove ellipsis (and last word)\n",
        "  text = re.sub(r\"(?<=\\w)-(?=\\w)\", \" \", text)         # Replace dash between words\n",
        "  text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)  # Remove punctuation\n",
        "\n",
        "  tokens = word_tokenize(text)                                            # Get tokens from text\n",
        "  tokens = [t for t in tokens if not t in stop_words]                  # Remove stopwords\n",
        "  tokens = [\"\" if t.isdigit() else t for t in tokens]                 # Remove digits\n",
        "  tokens = [t for t in tokens if len(t) > 1]                          # Remove short tokens\n",
        "  return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "hI4vZU365Ig3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _, example in zip(range(5), dataset[\"train_orig\"]):\n",
        "  print(example[\"sentence\"], example[\"label\"])\n",
        "  print(preproc(example[\"sentence\"]))"
      ],
      "metadata": {
        "id": "x-R9MBrZ-mFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "a8daCxQsDQw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences, train_labels = [], []\n",
        "for example in dataset[\"train_orig_eda_embedding_wordnet\"]:\n",
        "  train_sentences.append(preproc(example[\"sentence\"]))\n",
        "  train_labels.append(int(example[\"label\"]))"
      ],
      "metadata": {
        "id": "eqaXsXHqDZlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences, test_labels = [], []\n",
        "for example in dataset[\"val\"]:\n",
        "  test_sentences.append(preproc(example[\"sentence\"]))\n",
        "  test_labels.append(int(example[\"label\"]))"
      ],
      "metadata": {
        "id": "R7tu2aupE1f7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels[:5]"
      ],
      "metadata": {
        "id": "izESHAaWDpnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences[:5]"
      ],
      "metadata": {
        "id": "3oKdEimADn-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Eki4Mn90rd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_pip = Pipeline([\n",
        "    # (\"vect\", CountVectorizer()),\n",
        "    (\"tf-idf\", TfidfVectorizer()),\n",
        "    (\"nb\", MultinomialNB()),\n",
        "])\n",
        "\n",
        "nb_pip.fit(train_sentences, train_labels)\n",
        "acc = np.mean(nb_pip.predict(test_sentences) == test_labels)\n"
      ],
      "metadata": {
        "id": "zW-F92Z3CWb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_pip = Pipeline([\n",
        "    # (\"vect\", CountVectorizer()),\n",
        "    (\"tf-idf\", TfidfVectorizer()),\n",
        "    (\"nb\", BernoulliNB()),\n",
        "])\n",
        "\n",
        "nb_pip.fit(train_sentences, train_labels)\n",
        "acc = np.mean(nb_pip.predict(test_sentences) == test_labels)\n"
      ],
      "metadata": {
        "id": "fsEfI1Tt8Vyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results = nb_pip.predict(test_sentences)\n",
        "tp = np.sum((test_results == 1) & (np.array(test_labels) == 1))\n",
        "fp = np.sum((test_results == 1) & (np.array(test_labels) == 0))\n",
        "tn = np.sum((test_results == 0) & (np.array(test_labels) == 0))\n",
        "fn = np.sum((test_results == 0) & (np.array(test_labels) == 1))\n",
        "\n",
        "acc = (tp + tn) / (tp + fp + tn + fn)\n",
        "prec = tp / (tp + fp)\n",
        "rec = tp / (tp + fn)\n",
        "f1 = 2 * prec * rec / (prec + rec)"
      ],
      "metadata": {
        "id": "xkYuYS6BDLsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc, prec, rec, f1"
      ],
      "metadata": {
        "id": "Nm5cyGC6EyCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [preproc(example[\"sentence\"]) for example in dataset[\"train_orig\"]]"
      ],
      "metadata": {
        "id": "eOKJqAkTXClC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_ = [\" \".join(example) for example in corpus]"
      ],
      "metadata": {
        "id": "BATQwc5z6PH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "X = vectorizer.fit_transform(train_sentences)\n",
        "vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "id": "wPedfuwgGOHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset[\"train_orig\"])"
      ],
      "metadata": {
        "id": "OIPZCL893Pv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "IVB4hF4AHD0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "tp = fp = tn = fn = 0\n",
        "\n",
        "for _ in range(10):\n",
        "  km = Pipeline([\n",
        "      (\"tf-idf\", TfidfVectorizer()),\n",
        "      (\"kmeans\", KMeans(n_clusters=2, n_init=\"auto\")),\n",
        "  ])\n",
        "  km.fit(train_sentences, train_labels)\n",
        "  test_results = km.predict(test_sentences)\n",
        "\n",
        "  tp += np.sum((test_results == 1) & (np.array(test_labels) == 1))\n",
        "  fp += np.sum((test_results == 1) & (np.array(test_labels) == 0))\n",
        "  tn += np.sum((test_results == 0) & (np.array(test_labels) == 0))\n",
        "  fn += np.sum((test_results == 0) & (np.array(test_labels) == 1))\n",
        "\n",
        "\n",
        "acc = (tp + tn) / (tp + fp + tn + fn)\n",
        "prec = tp / (tp + fp)\n",
        "rec = tp / (tp + fn)\n",
        "f1 = 2 * prec * rec / (prec + rec)\n",
        "\n",
        "acc, prec, rec, f1"
      ],
      "metadata": {
        "id": "BohrcrwsF84E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import MiniBatchKMeans\n",
        "\n",
        "km = MiniBatchKMeans(n_clusters=2).fit(X)\n",
        "km.labels_\n",
        "acc = (km.labels_ == [example[\"label\"] for example in dataset[\"train_orig\"]]).mean()\n",
        "print(f\"Accuracy: {acc}\")"
      ],
      "metadata": {
        "id": "Q4p7T4DmVaYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import MeanShift\n",
        "\n",
        "km = MeanShift().fit(X)\n",
        "km.labels_"
      ],
      "metadata": {
        "id": "tmCZiecJN86L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import MiniBatchKMeans\n",
        "\n",
        "mkm = MiniBatchKMeans(n_clusters=2).fit(X)\n",
        "mkm.labels_\n",
        "acc = (mkm.labels_ == [example[\"label\"] for example in dataset[\"train\"]]).mean()\n",
        "print(f\"Accuracy: {acc}\")"
      ],
      "metadata": {
        "id": "gAPlPU1A6sde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "db = DBSCAN().fit(X)\n",
        "np.max(db.labels_)\n",
        "acc = (db.labels_ == [example[\"label\"] for example in dataset[\"train\"]]).mean()\n",
        "print(f\"Accuracy: {acc}\")"
      ],
      "metadata": {
        "id": "wJAbXSKtF9wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "L6k3QAFeHkDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "lHFoKDDcWdvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = api.load(\"fasttext-wiki-news-subwords-300\")"
      ],
      "metadata": {
        "id": "szSq1Zld7l7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v = model"
      ],
      "metadata": {
        "id": "oJXrMnSbJXQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.vector_size"
      ],
      "metadata": {
        "id": "lnp44FfbImQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.most_similar(\"movie\")"
      ],
      "metadata": {
        "id": "4SxM-E9QLIPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trains = []\n",
        "for sent in train_sentences:\n",
        "  vecs = []\n",
        "  for word in sent.split():\n",
        "    if word in model:\n",
        "      vecs.append(model[word])\n",
        "\n",
        "  avg = sum(vecs)/len(vecs) if len(vecs) > 0 else np.zeros(model.vector_size)\n",
        "  trains.append(avg)\n",
        "X = np.array(trains)"
      ],
      "metadata": {
        "id": "p4k1EhD2ILh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "yPnTU0kEL3l9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[0]"
      ],
      "metadata": {
        "id": "kOwYvMCgL4kZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import NearestCentroid\n",
        "nc_pip = Pipeline([\n",
        "    # (\"vect\", CountVectorizer()),\n",
        "    (\"tf-idf\", TfidfVectorizer()),\n",
        "    (\"nc\", NearestCentroid(n_jobs=1)),\n",
        "])\n",
        "\n",
        "nc_pip.fit(train_sentences, train_labels)"
      ],
      "metadata": {
        "id": "vSp1b6pjN7nC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(nc_pip.predict(test_sentences) == test_labels)"
      ],
      "metadata": {
        "id": "NZetfHh-S0ua"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}